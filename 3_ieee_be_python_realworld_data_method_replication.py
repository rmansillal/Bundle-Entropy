# -*- coding: utf-8 -*-
"""3_IEEE_BE_Python_RealWorld_Data_Method_Replication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1woNGSpYD6jR3JCaDxqlX8dknUHgQfwrF

# *Mansilla, R., Smith, G., Smith A. and Goulding, J. "Bundle entropy as an optimized measure of consumersâ€™ systematic product choice combinations in mass transactional data". In 2022 IEEE International Conference on Big Data (IEEE Big Data 2022), Osaka, Japan.*
-----

# **DUNNHUMBY**

# **Code and Replication of method on real-world data**

* Assumes a running postgres database with hostname: \<hostname\>, database: \<database\>
* Assumes the open real-world dataset [The complete journey data from dunnhumby](https://www.dunnhumby.com/source-files/) has been loaded into the database in a table called `transaction_data` in postgres.

## 1) Connect to database & Import Python libraries
"""

# Import libraries
import numpy as np
import pandas as pd
from getpass import getpass
from scipy.stats import kendalltau
import matplotlib.pyplot as plt
from pylab import rcParams
import seaborn as sns
import matplotlib.lines as lines
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Commented out IPython magic to ensure Python compatibility.
# Connect to database
# %config Completer.use_jedi = False
# %reload_ext sql

# %sql {f"postgresql://<username>:{getpass()}@<hostname>/<database>"}

"""# SQL

### 2) SQL code to create tables using the 2,500 customers
"""

# Commented out IPython magic to ensure Python compatibility.
# %%sql
# --======================================================================  
# --============= DUNMHUMBY CODE TO COMPUTE ENTROPIES ====================
# --======================================================================
# -- Dunnhumby trasnaction_data has data 711 days (around to years).
# 
# --==================================
# --============= First =============
# --==================================
# CREATE TABLE rm.DUNN_active_cust_measures_norm AS (
# 		   SELECT household_key,
#                   round(bev3_norm(basket_id::TEXT, product_id::TEXT),2) as BE,
#                   round(norm_entropy(product_id::TEXT),2) as IE,
#                   round(joint_entropy(basket_id::TEXT, product_id::TEXT),2) as BLE,
#                   round(bre(basket_id::TEXT, product_id::TEXT,0.10 ),2) as BRE10,
#                   round(bre(basket_id::TEXT, product_id::TEXT,0.24 ),2) as BRE24,
#                   round(bre(basket_id::TEXT, product_id::TEXT,0.70 ),2) as BRE70
#                   FROM rm.trans_data_cust_21_purch
#                   GROUP BY 1
#                   ORDER BY 1)
# 
# --==================================
# --============= Second ============= 
# --==================================
# CREATE TABLE rm.DUNN_ct_bkts_and_dist_bkts AS (
#       SELECT household_key, COUNT( DISTINCT basket_composition) unique_baskets
#     FROM (
#         SELECT household_key, basket_id, STRING_AGG ( product_id::TEXT, ',' ORDER BY product_id ) basket_composition
#         FROM rm.trans_data_cust_21_purch
#         GROUP BY 1,2
#     ) x
#     GROUP BY 1)
# --==================================
# --============= Third ============= 
# --==================================
# CREATE TABLE rm.DUNN_mean_median_gap_and_periodindays AS (
# SELECT household_key, MAX(day) - MIN(day) AS period_covered_days, AVG(gap) as mean_gap, PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY gap) as median_gap
#             FROM (
#             SELECT household_key, day, day - lag(day) OVER (PARTITION BY household_key ORDER BY day) AS gap
#             FROM (SELECT household_key, basket_id, day FROM transaction_data GROUP BY 1,2,3) x
#                 ) y
#             GROUP BY 1)
# 
# --==================================
# --============= Fourth ==============
# --==================================
# CREATE TABLE rm.DUNN_dunnhumby_all_measures_norm AS (
# SELECT *
# FROM rm.DUNN_active_cust_measures_norm
# JOIN rm.DUNN_ct_bkts_and_dist_bkts
# USING (household_key)
# JOIN rm.DUNN_mean_median_gap_and_periodindays
# USING (household_key))
#

"""# PYTHON

### 3) FIGURE 4 of our paper

#### 3.1 Kendall Tau Rank Agreement (Mean Rank Difference)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #-- Load DUNNHUMBY data
# data_dunnhumby = """SELECT *
#                       FROM rm.DUNN_dunnhumby_all_measures_norm
#                     """
# data_dunnhumby = %sql {data_dunnhumby}
# data_dunnhumby = data_dunnhumby.DataFrame().fillna(0)

data_dunnhumby.head()

#-- Function to plot Kendall Tau Rank
def plt_kendal_tau( data, title = None ):
    
    measures = ['be','ie', 'ble', 'bre10',  'bre24','bre70' ]

    all_lists = []

    for m in measures:
        a = data[['household_key', m]].values
        idx = np.asarray([np.asarray(range(data.shape[0]))]).T
        a = np.append(idx,a[a[:, 1].argsort()],axis =1)[:,(0,1)]
        all_lists.append( a[a[:, 1].argsort()][:,0] )

    out = np.zeros((6,6),dtype=float)
    out_p = np.zeros((6,6),dtype=float)
    out_mrd = np.zeros((6,6),dtype=float)
    for row, l1 in enumerate(all_lists):
        for col, l2 in enumerate(all_lists):
            tau, p_value = kendalltau(l1,l2,variant='b')
            out[row, col] = tau
            out_p[row, col] = p_value
            out_mrd[row,col] = np.mean(np.abs(l1 - l2))#/len(l1))

    mask = np.zeros_like(np.zeros((6,6)), dtype=bool)
    mask[np.triu_indices_from(mask)] = True
    for i in range(6):
        for j in range(6):
            if i == j:
                mask[i,j] = 0

    ax = sns.heatmap(out, annot=False, xticklabels = ['BE', 'IE', 'BLE', 'BRE10', 'BRE24', 'BRE70'],  mask=mask)
    
    plt.yticks(np.arange(6)+0.5,['BE', 'IE', 'BLE', 'BRE10', 'BRE24', 'BRE70'],
           rotation=0, fontsize="10", va="center")

    if not title is None:
        _ = ax.set_title(title)
    
    for y in range(out_mrd.shape[0]):
        for x in range(out_mrd.shape[1]):
            if (y == x) or (y == 5 and x == 2):
                plt.text(x + 0.5, y + 0.5, '{:.2f}\n({:.0f})'.format(out[y,x],out_mrd[y, x]),
                        horizontalalignment='center',
                        verticalalignment='center',
                        c= 'black'
                        )
            else:
                plt.text(x + 0.5, y + 0.5, '{:.2f}\n({:.0f})'.format(out[y,x],out_mrd[y, x]),
                    horizontalalignment='center',
                    verticalalignment='center',
                    c= 'white'
                    )

#-- Plot Kendall Tau Rank Agreement
plt_kendal_tau( data = data_dunnhumby )

"""### 4) TABLE 2 of our paper"""

#-- compute life value (Mean Spend per Month)
data = data_dunnhumby
data['lifetime_value'] = np.asarray(data.total_spend,dtype=float)/np.asarray(data.period_covered_days,dtype=float)

from scipy.stats import pearsonr

vars_of_interest = ['avg_basket_spend', 'lifetime_value', 'num_visits']
measures = ['be', 'bre10', 'bre24', 'bre70', 'ie', 'ble']
pvals = []
vset = []
for voi in vars_of_interest:
    print()
    print(voi)
    r = []
    for m in measures:
        v, p = pearsonr(np.asarray(data[m],dtype=float), np.asarray(data[voi],dtype=float))
        print(f'   {m}:{v}')
        pvals.append(p)
        vset.append(v)

from statsmodels.stats.multitest import multipletests

a = multipletests(pvals, method = 'fdr_bh',alpha=0.01)
list(zip(a[0],vset))

from statsmodels.stats.multitest import multipletests
import statsmodels
statsmodels.__version__

